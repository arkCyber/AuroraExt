# LLaMA.cpp

Aurora supports LLaMA.cpp API endpoints. You can use any LLaMA.cpp API endpoint with Aurora.

## Adding LLaMA.cpp API

1. Click on the Aurora icon on the browser toolbar.

2. Click on the `Settings` icon.

3. Go to the `OpenAI Compatible API` tab.

4. Click on the `Add Provider` button.

5. Select `LLaMA.cpp` from the dropdown.

6. Enter the `LLaMA.cpp URL`. (by default it is `http://localhost:8080/v1`)

7. Click on the `Save` button.


::: info
You don't need to add any models since Aurora will automatically fetch them from the LLaMA.cpp instance you have configured.

The model must be loaded in LLaMA.cpp before Aurora can fetch it.
:::